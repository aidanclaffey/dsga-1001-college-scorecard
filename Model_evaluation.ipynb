{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation for college scorecard - earnings prediction\n",
    "\n",
    "This notebook will use several different supervised learning regression algorithms to model earnings after college using College Scorecard data. The models included for evaluation will be:\n",
    "\n",
    "1. Linear Regression\n",
    "1. Decision Tree\n",
    "1. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "seed = 12345\n",
    "\n",
    "#Load the data into a data frame\n",
    "data file = ''\n",
    "data = pd.read_csv(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "We need to split the data into training and test sets. Also, we need to think about possibly centering/normalizing the data.\n",
    "\n",
    "Normalizing may make sense at least for linear regression, so we can understand the features a little bit better\n",
    "\n",
    "Also, we can think about using PCA (if only for visualization purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data into training and test\n",
    "# Need to figure out best way to split time series data\n",
    "# Do we split based only on colleges (i.e. each college is either train or test)\n",
    "# Do we split based on college and year (i.e. each data entry is either train or test)\n",
    "# Another way to split?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "First try: no feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "print('R^2 of training set: ' + lin_reg.score())\n",
    "print('R^2 of test set: ' + lin_reg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second try: feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "X_train_s, y_train_s, X_test_s, y_test_s = ???\n",
    "\n",
    "lin_reg_scaled = LinearRegression()\n",
    "lin_reg_scaled.fit(X_train_s, y_train_s)\n",
    "print('R^2 of training set: ' + lin_reg_scaled.score())\n",
    "print('R^2 of test set: ' + lin_reg_scaled.score(X_test_s, y_test_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "First using default params, then tuning params\n",
    "We can think also use different criterion for splitting (MSE vs MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "#Do decision tree using out of box parameters\n",
    "dec_tree = DecisionTreeRegressor(criterion = 'mse', random_state = seed)\n",
    "dec_tree.fit(X_train, y_train)\n",
    "\n",
    "# Train and test accuracy\n",
    "train_accuracy = model.score(X_train, y_train)\n",
    "test_accuracy = model.score(X_test, y_test)\n",
    "print('mse')\n",
    "print('training accuracy:\\t', train_accuracy)\n",
    "print('test accuracy:\\t\\t', test_accuracy)\n",
    "\n",
    "#print top 20 feature importance\n",
    "x = np.arange(20)\n",
    "sorted_inds = np.argsort(-(model.feature_importances_))[:20]\n",
    "sorted_colnames = df.columns[sorted_inds]\n",
    "fig = plt.figure(figsize = (10,5))\n",
    "plt.bar(x,model.feature_importances_[sorted_inds], width = 0.8)\n",
    "plt.xticks(x,(sorted_colnames))\n",
    "plt.show()\n",
    "\n",
    "#Repeat decision tree using out of box parameters with 'mae' as criterion\n",
    "dec_tree = DecisionTreeRegressor(criterion = 'mae', random_state = seed)\n",
    "dec_tree.fit(X_train, y_train)\n",
    "\n",
    "# Train and test accuracy\n",
    "train_accuracy = model.score(X_train, y_train)\n",
    "test_accuracy = model.score(X_test, y_test)\n",
    "print('mae')\n",
    "print('training accuracy:\\t', train_accuracy)\n",
    "print('test accuracy:\\t\\t', test_accuracy)\n",
    "\n",
    "#print top 20 feature importance\n",
    "x = np.arange(20)\n",
    "sorted_inds = np.argsort(-(model.feature_importances_))[:20]\n",
    "sorted_colnames = df.columns[sorted_inds]\n",
    "fig = plt.figure(figsize = (10,5))\n",
    "plt.bar(x,model.feature_importances_[sorted_inds], width = 0.8)\n",
    "plt.xticks(x,(sorted_colnames))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_samples_split_range = np.arange(??,??,??)\n",
    "min_samples_leaf_range = np.arange(??,??,??)\n",
    "\n",
    "# Function that takes model and outputs R^2\n",
    "def get_r2(criterion, min_split,min_leaf):\n",
    "    model = DecisionTreeRegressor(criterion = criterion, random_state = seed, \n",
    "                                  min_samples_split = min_split,\n",
    "                                  min_samples_leaf = min_leaf)\n",
    "    model.fit(X_train,y_train)\n",
    "    return(model.score(X_test, y_test))\n",
    "\n",
    "r2_array = dict{\n",
    "    'mse' : np.zeros([10,10]),\n",
    "    'mae' : np.zeros([10,10])}\n",
    "#This for loop runs the model\n",
    "for crit in ['mse', 'mae']:\n",
    "    for i in range(0,10):\n",
    "        min_samples_split = min_samples_split_values[i]\n",
    "        for j in range(0,10):\n",
    "            min_samples_leaf = min_samples_leaf_values[j]\n",
    "            r2_array[crit][i,j]=get_r2(crit, min_samples_split,min_samples_leaf)\n",
    "\n",
    "#plot the models\n",
    "fig = plt.figure(figsize = (10,4))\n",
    "for i in range(0,10):\n",
    "    plt.plot(min_samples_split_values, r2_array['mse'][:,i], \n",
    "             label = 'min_leaf: ' + str(min_samples_leaf_values[i]))\n",
    "plt.legend(loc=(1.01,0.25))\n",
    "plt.xticks(min_samples_split_values)\n",
    "plt.xlabel('min_samples_split')\n",
    "plt.title('R^2 for MSE decision tree')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize = (10,4))\n",
    "for i in range(0,10):\n",
    "    plt.plot(min_samples_split_values, r2_array['mae'][:,i], \n",
    "             label = 'min_leaf: ' + str(min_samples_leaf_values[i]))\n",
    "plt.legend(loc=(1.01,0.25))\n",
    "plt.xticks(min_samples_split_values)\n",
    "plt.xlabel('min_samples_split')\n",
    "plt.title('R^2 for MASE decision tree')\n",
    "plt.show()\n",
    "print(np.max(r2_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First using out-of-box parameters (which will probably lead to overfitting)\n",
    "#use mse and then use mae\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rand_forest = RandomForestRegressor(criterion = 'mse',oob_score = True, random_state = seed)\n",
    "rand_forest.fit(X_train, y_train)\n",
    "print('mse')\n",
    "print('OOB score : ' + rand_forest.oob_score)\n",
    "print('Train accuracy: ' + rand_forest.score(X_train, y_train))\n",
    "print('Test accuracy: ' + rand_forest.score(X_test, y_test))\n",
    "print()\n",
    "rand_forest = RandomForestRegressor(criterion = 'mae',oob_score = True, random_state = seed)\n",
    "rand_forest.fit(X_train, y_train)\n",
    "print('mae')\n",
    "print('OOB score : ' + rand_forest.oob_score)\n",
    "print('Train accuracy: ' + rand_forest.score(X_train, y_train))\n",
    "print('Test accuracy: ' + rand_forest.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change some parameters and look for improvement (use better of mae and mse)\n",
    "\n",
    "#first change n_estimators (to 100 because that is sklearn's version 0.22 default)\n",
    "# and change max_features to 0.66 (since that is what wikipedia claims is a good starting point\n",
    "# for a random forest regression)\n",
    "n_estimators = 100\n",
    "max_features = 0.66\n",
    "\n",
    "rand_forest = RandomForestRegressor(n_estimators = n_estimators, max_features = max_features,\n",
    "                                    criterion = 'mae', oob_score = True, random_state = seed)\n",
    "rand_forest.fit(X_train, y_train)\n",
    "print(rand_forest.get_params())\n",
    "print('OOB score : ' + rand_forest.oob_score)\n",
    "print('Train accuracy: ' + rand_forest.score(X_train, y_train))\n",
    "print('Test accuracy: ' + rand_forest.score(X_test, y_test))\n",
    "\n",
    "#change other parameters as we see fit (probably max_depth, min_size_leaf, min_size_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
